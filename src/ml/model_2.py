import os
import pandas as pd
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from urllib.parse import urlparse
import boto3
from botocore.client import Config
import io

# ----- CONFIG -----
CSV_PATH = './data/dataset.csv'
BUCKET_NAME = 'images-bucket'
MODEL_OUTPUT_PATH = 'model_state_dict.pth'
MINIO_ENDPOINT = 'http://localhost:9000'
ACCESS_KEY = 'minioadmin'
SECRET_KEY = 'minioadmin'
BATCH_SIZE = 4
EPOCHS = 3

print("üîß Initialisation du client MinIO...")
s3 = boto3.client(
    's3',
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY,
    config=Config(signature_version='s3v4'),
    region_name='us-east-1'
)

# ----- Dataset depuis MinIO -----
class S3ImageDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform
        self.label_map = {label: idx for idx, label in enumerate(df['label'].unique())}
        self.reverse_map = {v: k for k, v in self.label_map.items()}

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        parsed = urlparse(row['image_path'])
        bucket = parsed.netloc
        key = parsed.path.lstrip('/')
        label = self.label_map[row['label']]

        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            img_data = obj['Body'].read()
            image = Image.open(io.BytesIO(img_data)).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f"[ERREUR] Image ignor√©e : {key} ‚Üí {e}")
            return None  # DataLoader filtrera avec collate_fn

# ----- Lecture du dataset -----
print(f"üìÑ Lecture du fichier CSV : {CSV_PATH}")
df = pd.read_csv(CSV_PATH)
print(f"üîç {len(df)} entr√©es trouv√©es dans le CSV.")

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

print("üì• Chargement des images depuis MinIO...")
raw_dataset = S3ImageDataset(df, transform=transform)

# ----- DataLoader avec filtrage √† la vol√©e -----
def custom_collate(batch):
    return [b for b in batch if b is not None]

loader = DataLoader(raw_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)

# V√©rifier que le DataLoader contient des donn√©es
sample_batch = next(iter(loader), [])
if len(sample_batch) < 2:
    print("‚ùå Pas assez d‚Äôimages valides pour entra√Æner.")
    exit()
print(f"‚úÖ DataLoader pr√™t avec batch_size={BATCH_SIZE}")

# ----- Mod√®le -----
print("üß† Chargement du mod√®le ResNet34...")
model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
model.fc = torch.nn.Linear(model.fc.in_features, len(raw_dataset.label_map))
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ----- Entra√Ænement -----
print("üöÄ D√©but de l'entra√Ænement...")
model.train()
for epoch in range(EPOCHS):
    total_loss = 0.0
    for batch in loader:
        inputs, labels = zip(*batch)
        inputs = torch.stack(inputs)
        labels = torch.tensor(labels)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"üìà Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

# ----- Sauvegarde du mod√®le -----
print("üíæ Sauvegarde du mod√®le localement...")
os.makedirs("temp_model", exist_ok=True)
torch.save(model.state_dict(), f"temp_model/{MODEL_OUTPUT_PATH}")

print("‚òÅÔ∏è Upload du mod√®le dans MinIO...")
s3.upload_file(f"temp_model/{MODEL_OUTPUT_PATH}", BUCKET_NAME, f"models/{MODEL_OUTPUT_PATH}")
print(f"‚úÖ Mod√®le sauvegard√© dans MinIO : models/{MODEL_OUTPUT_PATH}")
